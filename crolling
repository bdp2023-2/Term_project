import nest_asyncio
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import csv

nest_asyncio.apply()

# nltk 라이브러리에서 불용어 다운로드
nltk.download('stopwords')
nltk.download('punkt')

# 불용어 리스트
stop_words = set(stopwords.words('english'))

# 논문 정보를 저장할 리스트
all_abstracts = []

async def fetch_page(session, url):
    async with session.get(url) as response:
        return await response.text()

async def scrape_arxiv(page):
    url = f"https://arxiv.org/search/?query=AI&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start={50 * (page - 1)}"
    
    async with aiohttp.ClientSession() as session:
        response = await fetch_page(session, url)
        
        soup = BeautifulSoup(response, 'html.parser')
        paper_links = soup.find_all('p', class_='list-title is-inline-block')
        
        for link in paper_links:
            paper_url = link.a['href']
            paper_response = await fetch_page(session, paper_url)
            paper_soup = BeautifulSoup(paper_response, 'html.parser')
            abstract_tag = paper_soup.find('blockquote', class_='abstract mathjax')
            
            if abstract_tag:
                abstract_text = abstract_tag.get_text(separator=' ', strip=True)
                
                # 불용어 처리
                words = word_tokenize(abstract_text)
                filtered_abstract = ' '.join([word for word in words if word.lower() not in stop_words])
                
                all_abstracts.append(filtered_abstract)
            else:
                all_abstracts.append("Abstract not found on the page.")

async def main():
    page = 1
    
    while True:
        await scrape_arxiv(page)
        page += 1

        # 전체 페이지 크롤링
        if page > 100:  # 예시로 100페이지까지 크롤링
            break

# 비동기 코드 실행
await main()

# 결과를 CSV 파일로 저장
with open('filtered_abstracts.csv', 'w', newline='', encoding='utf-8') as csvfile:
    csv_writer = csv.writer(csvfile)
    csv_writer.writerow(['Filtered Abstract'])
    csv_writer.writerows([[abstract] for abstract in all_abstracts])
