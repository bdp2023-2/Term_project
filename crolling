import nest_asyncio
nest_asyncio.apply()

import aiohttp
import asyncio
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# nltk 라이브러리에서 불용어 다운로드
nltk.download('stopwords')
nltk.download('punkt')

# 불용어 리스트
stop_words = set(stopwords.words('english'))

# 논문 정보를 저장할 리스트
all_abstracts = []

async def fetch_page(session, url):
    async with session.get(url) as response:
        return await response.text()

async def scrape_arxiv(page):
    url = f"https://arxiv.org/search/?query=AI&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start={50 * (page - 1)}"
    
    async with aiohttp.ClientSession() as session:
        response = await fetch_page(session, url)
        
        soup = BeautifulSoup(response, 'html.parser')
        paper_links = soup.find_all('p', class_='list-title is-inline-block')
        
        for link in paper_links:
            paper_url = link.a['href']
            paper_response = await fetch_page(session, paper_url)
            paper_soup = BeautifulSoup(paper_response, 'html.parser')
            abstract_tag = paper_soup.find('blockquote', class_='abstract mathjax')
            
            if abstract_tag:
                abstract_text = abstract_tag.get_text(separator=' ', strip=True)
                
                # 불용어 처리
                words = word_tokenize(abstract_text)
                filtered_abstract = ' '.join([word for word in words if word.lower() not in stop_words])
                
                all_abstracts.append(filtered_abstract)
            else:
                all_abstracts.append("Abstract not found on the page.")

async def main():
    tasks = [scrape_arxiv(page) for page in range(1, 6)]  # 예시로 5페이지까지 비동기로 크롤링
    
    # 비동기로 페이지 요청
    await asyncio.gather(*tasks)

# 비동기 코드 실행
await main()

# 결과 출력
for idx, abstract in enumerate(all_abstracts, start=1):
    print(f"Filtered Abstract {idx}:", abstract)
